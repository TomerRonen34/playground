{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use xgboost version 0.9, otherwise you might get an error when using RFE feature selector\n",
    "### pip install --upgrade xgboost==0.90"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NHANES I Survival Model\n",
    "\n",
    "This is a cox proportional hazards model on data from <a href=\"https://wwwn.cdc.gov/nchs/nhanes/nhanes1\">NHANES I</a> with followup mortality data from the <a href=\"https://wwwn.cdc.gov/nchs/nhanes/nhefs\">NHANES I Epidemiologic Followup Study</a>. It is designed to illustrate how SHAP values enable the interpretion of XGBoost models with a clarity traditionally only provided by linear models. We see interesting and non-linear patterns in the data, which suggest the potential of this approach. Keep in mind the data has not yet been checked by us for calibrations to current lab tests and so you should not consider the results as actionable medical insights, but rather a proof of concept. \n",
    "\n",
    "Note that support for Cox loss and SHAP interaction effects were only recently merged, so you will need the latest master version of XGBoost to run this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "import xgboost\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pylab as pl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data\n",
    "\n",
    "This uses a pre-processed subset of NHANES I data available in the SHAP datasets module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y = shap.datasets.nhanesi()\n",
    "X_display,y_display = shap.datasets.nhanesi(display=True) # human readable feature values\n",
    "\n",
    "# create a train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vanilla XGBoost model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-cox-nloglik:7.12869\n",
      "[20]\tvalidation_0-cox-nloglik:6.55299\n",
      "[40]\tvalidation_0-cox-nloglik:6.50416\n",
      "[60]\tvalidation_0-cox-nloglik:6.49395\n",
      "[80]\tvalidation_0-cox-nloglik:6.49788\n",
      "[99]\tvalidation_0-cox-nloglik:6.51104\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "             colsample_bynode=1, colsample_bytree=1, eta=0.002, gamma=0,\n",
       "             importance_type='gain', learning_rate=0.1, max_delta_step=0,\n",
       "             max_depth=3, min_child_weight=1, missing=None, n_estimators=100,\n",
       "             n_jobs=1, nthread=None, objective='survival:cox', random_state=34,\n",
       "             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=34,\n",
       "             silent=None, subsample=0.5, verbosity=1)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit_params = {\n",
    "    \"eta\": 0.002,\n",
    "    \"max_depth\": 3, \n",
    "    \"objective\": \"survival:cox\",\n",
    "    \"subsample\": 0.5,\n",
    "    \"n_estimators\": 100,\n",
    "    \"random_state\": 34,\n",
    "    \"seed\": 34\n",
    "}\n",
    "orig_model = XGBRegressor(**fit_params)\n",
    "orig_model.fit(X_train, y_train, eval_set=[(X_test, y_test)], verbose=int(fit_params[\"n_estimators\"]/5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost model with SHAP feature importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import shap\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "\n",
    "class ShapXGBRegressor(XGBRegressor):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.vanilla_model = XGBRegressor(**kwargs)\n",
    "        self.feature_importances = None\n",
    "\n",
    "    @property\n",
    "    def feature_importances_(self):\n",
    "        return self.feature_importances\n",
    "    \n",
    "    @property\n",
    "    def vanilla_feature_importances(self):\n",
    "        return super().feature_importances_\n",
    "\n",
    "    def fit(self, X, y, **kwargs):\n",
    "        super().fit(X, y, **kwargs)\n",
    "        self.vanilla_model.__dict__ = self.__dict__\n",
    "        self._calculate_feature_importances(X)\n",
    "        return self\n",
    "\n",
    "    def _calculate_feature_importances(self, X):\n",
    "        explainer = shap.TreeExplainer(self.vanilla_model)\n",
    "        shap_values = explainer.shap_values(X)\n",
    "        feature_importances = np.mean(np.abs(shap_values), axis=0)\n",
    "        self.feature_importances = feature_importances\n",
    "\n",
    "    def get_params(self, deep=False):\n",
    "        return self.vanilla_model.get_params(deep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-cox-nloglik:7.12869\n",
      "[20]\tvalidation_0-cox-nloglik:6.55299\n",
      "[40]\tvalidation_0-cox-nloglik:6.50416\n",
      "[60]\tvalidation_0-cox-nloglik:6.49395\n",
      "[80]\tvalidation_0-cox-nloglik:6.49788\n",
      "[99]\tvalidation_0-cox-nloglik:6.51104\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting feature_perturbation = \"tree_path_dependent\" because no background data was given.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ShapXGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "                 colsample_bynode=1, colsample_bytree=1, eta=0.002, gamma=0,\n",
       "                 importance_type='gain', learning_rate=0.1, max_delta_step=0,\n",
       "                 max_depth=3, min_child_weight=1, missing=None,\n",
       "                 n_estimators=100, n_jobs=1, nthread=None,\n",
       "                 objective='survival:cox', random_state=34, reg_alpha=0,\n",
       "                 reg_lambda=1, scale_pos_weight=1, seed=34, silent=None,\n",
       "                 subsample=0.5, verbosity=1)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_model = ShapXGBRegressor(**fit_params)\n",
    "my_model.fit(X_train, y_train, eval_set=[(X_test, y_test)], verbose=int(fit_params[\"n_estimators\"]/5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare feature importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(my_model.vanilla_feature_importances == orig_model.feature_importances_).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.02837274, 0.3421058 , 0.04055583, 0.03419708, 0.01927379,\n",
       "       0.02602945, 0.04228061, 0.041077  , 0.03282125, 0.02964063,\n",
       "       0.03748016, 0.02886848, 0.07809756, 0.04533925, 0.02949669,\n",
       "       0.0309557 , 0.04009185, 0.03375924, 0.03955674], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orig_model.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.02230697, 1.2602944 , 0.09946342, 0.11448295, 0.00693359,\n",
       "       0.0219535 , 0.0549446 , 0.04781595, 0.05127395, 0.03270031,\n",
       "       0.05935754, 0.04265623, 0.29262108, 0.11732519, 0.02903319,\n",
       "       0.03753575, 0.08687518, 0.08802547, 0.04167419], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_model.feature_importances_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use models for feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "def impute(df):\n",
    "    imputer = SimpleImputer(strategy='median')\n",
    "    df_imputed = df.copy()\n",
    "    df_imputed[:] = imputer.fit_transform(df)\n",
    "    return df_imputed    \n",
    "\n",
    "def impute_train_test(X_train, X_test):\n",
    "    imputer = SimpleImputer(strategy='median')\n",
    "    imputer.fit(X_train)\n",
    "    \n",
    "    X_train_imputed = X_train.copy()\n",
    "    X_train_imputed[:] = imputer.transform(X_train)\n",
    "    \n",
    "    X_test_imputed = X_test.copy()\n",
    "    X_test_imputed[:] = imputer.transform(X_test)\n",
    "    \n",
    "    return X_train_imputed, X_test_imputed\n",
    "\n",
    "X_train_imputed, X_test_imputed = impute_train_test(X_train, X_test)\n",
    "X_imputed = impute(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1  2  6 12 13]\n",
      "[False  True  True False False False  True False False False False False\n",
      "  True  True False False False False False]\n",
      "[15  1  1  3 13  8  1  2 12 11  6  7  1  1 14 10  4  5  9]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting feature_perturbation = \"tree_path_dependent\" because no background data was given.\n",
      "Setting feature_perturbation = \"tree_path_dependent\" because no background data was given.\n",
      "Setting feature_perturbation = \"tree_path_dependent\" because no background data was given.\n",
      "Setting feature_perturbation = \"tree_path_dependent\" because no background data was given.\n",
      "Setting feature_perturbation = \"tree_path_dependent\" because no background data was given.\n",
      "Setting feature_perturbation = \"tree_path_dependent\" because no background data was given.\n",
      "Setting feature_perturbation = \"tree_path_dependent\" because no background data was given.\n",
      "Setting feature_perturbation = \"tree_path_dependent\" because no background data was given.\n",
      "Setting feature_perturbation = \"tree_path_dependent\" because no background data was given.\n",
      "Setting feature_perturbation = \"tree_path_dependent\" because no background data was given.\n",
      "Setting feature_perturbation = \"tree_path_dependent\" because no background data was given.\n",
      "Setting feature_perturbation = \"tree_path_dependent\" because no background data was given.\n",
      "Setting feature_perturbation = \"tree_path_dependent\" because no background data was given.\n",
      "Setting feature_perturbation = \"tree_path_dependent\" because no background data was given.\n",
      "Setting feature_perturbation = \"tree_path_dependent\" because no background data was given.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1  3 12 13 16]\n",
      "[False  True False  True False False False False False False False False\n",
      "  True  True False False  True False False]\n",
      "[13  1  6  1 15 12  9  2  8 11  5  3  1  1 14  7  1  4 10]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "estimator = XGBRegressor(**fit_params)\n",
    "selector = RFE(estimator, n_features_to_select=5, step=1)\n",
    "selector = selector.fit(X_imputed, y)\n",
    "print(np.nonzero(selector.support_)[0])\n",
    "print(selector.support_)\n",
    "print(selector.ranking_)\n",
    "\n",
    "estimator = ShapXGBRegressor(**fit_params)\n",
    "selector = RFE(estimator, n_features_to_select=5, step=1)\n",
    "selector = selector.fit(X_imputed, y)\n",
    "print(np.nonzero(selector.support_)[0])\n",
    "print(selector.support_)\n",
    "print(selector.ranking_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argsort(orig_model.feature_importances_)[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argsort(my_model.feature_importances_)[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.columns[np.nonzero(selector.support_)[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_friedman1\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.svm import SVR\n",
    "X, y = make_friedman1(n_samples=50, n_features=10, random_state=0)\n",
    "estimator = ShapXGBRegressor(**params, random_state=34, seed=34)\n",
    "selector = RFE(estimator, 5, step=1)\n",
    "selector = selector.fit(X, y)\n",
    "print(selector.support_)\n",
    "print(selector.ranking_)\n",
    "\n",
    "X = X[:,::-1]\n",
    "estimator = ShapXGBRegressor(**params, random_state=34, seed=34)\n",
    "selector = RFE(estimator, 5, step=1)\n",
    "selector = selector.fit(X, y)\n",
    "print(selector.support_)\n",
    "print(selector.ranking_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explain the model's predictions on the entire dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_values = shap.TreeExplainer(model).shap_values(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SHAP Summary Plot\n",
    "\n",
    "The SHAP values for XGBoost explain the margin output of the model, which is the change in log odds of dying for a Cox proportional hazards model. We can see below that the primary risk factor for death according to the model is being old. The next most powerful indicator of death risk is being a man.\n",
    "\n",
    "This summary plot replaces the typical bar chart of feature importance. It tells which features are most important, and also their range of effects over the dataset. The color allows us match how changes in the value of a feature effect the change in risk (such that a high white blood cell count leads to a high risk of death)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values, X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SHAP Dependence Plots\n",
    "\n",
    "While a SHAP summary plot gives a general overview of each feature a SHAP dependence plot show how the model output varies by feauture value. Note that every dot is a person, and the vertical dispersion at a single feature value results from interaction effects in the model. The feature used for coloring is automatically chosen to highlight what might be driving these interactions. Later we will see how to check that the interaction is really in the model with SHAP interaction values. Note that the row of a SHAP summary plot results from projecting the points of a SHAP dependence plot onto the y-axis, then recoloring by the feature itself.\n",
    "\n",
    "Below we give the SHAP dependence plot for each of the NHANES I features, revealing interesting but expected trends. Keep in mind the calibration of some of these values can be different than a modern lab test so be careful drawing conclusions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we pass \"Age\" instead of an index because dependence_plot() will find it in X's column names for us\n",
    "# Systolic BP was automatically chosen for coloring based on a potential interaction to check that \n",
    "# the interaction is really in the model see SHAP interaction values below\n",
    "shap.dependence_plot(\"Age\", shap_values, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we pass display_features so we get text display values for sex\n",
    "shap.dependence_plot(\"Sex\", shap_values, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting show=False allows us to continue customizing the matplotlib plot before displaying it\n",
    "shap.dependence_plot(\"Systolic BP\", shap_values, X, show=False)\n",
    "pl.xlim(80,225)\n",
    "pl.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.dependence_plot(\"Poverty index\", shap_values, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.dependence_plot(\"White blood cells\", shap_values, X, display_features=X_display, show=False)\n",
    "pl.xlim(2,15)\n",
    "pl.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.dependence_plot(\"BMI\", shap_values, X, display_features=X_display, show=False)\n",
    "pl.xlim(15,50)\n",
    "pl.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.dependence_plot(\"Serum magnesium\", shap_values, X, show=False)\n",
    "pl.xlim(1.2,2.2)\n",
    "pl.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.dependence_plot(\"Sedimentation rate\", shap_values, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.dependence_plot(\"Serum protein\", shap_values, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.dependence_plot(\"Serum cholesterol\", shap_values, X, show=False)\n",
    "pl.xlim(100,400)\n",
    "pl.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.dependence_plot(\"Pulse pressure\", shap_values, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.dependence_plot(\"Serum iron\", shap_values, X, display_features=X_display)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.dependence_plot(\"TS\", shap_values, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.dependence_plot(\"Red blood cells\", shap_values, X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute SHAP Interaction Values\n",
    "\n",
    "See the Tree SHAP paper for more details, but briefly, SHAP interaction values are a generalization of SHAP values to higher order interactions. Fast exact computation of pairwise interactions are implemented in the latest version of XGBoost with the pred_interactions flag. With this flag XGBoost returns a matrix for every prediction, where the main effects are on the diagonal and the interaction effects are off-diagonal. The main effects are similar to the SHAP values you would get for a linear model, and the interaction effects captures all the higher-order interactions are divide them up among the pairwise interaction terms. Note that the sum of the entire interaction matrix is the difference between the model's current output and expected output, and so the interaction effects on the off-diagonal are split in half (since there are two of each). When plotting interaction effects the SHAP package automatically multiplies the off-diagonal values by two to get the full interaction effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes a couple minutes since SHAP interaction values take a factor of 2 * # features\n",
    "# more time than SHAP values to compute, since this is just an example we only explain\n",
    "# the first 2,000 people in order to run quicker\n",
    "shap_interaction_values = shap.TreeExplainer(model).shap_interaction_values(X.iloc[:2000,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SHAP Interaction Value Summary Plot\n",
    "\n",
    "A summary plot of a SHAP interaction value matrix plots a matrix of summary plots with the main effects on the diagonal and the interaction effects off the diagonal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_interaction_values, X.iloc[:2000,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SHAP Interaction Value Dependence Plots\n",
    "\n",
    "Running a dependence plot on the SHAP interaction values a allows us to separately observe the main effects and the interaction effects.\n",
    "\n",
    "Below we plot the main effects for age and some of the interaction effects for age. It is informative to compare the main effects plot of age with the earlier SHAP value plot for age. The main effects plot has no vertical dispersion because the interaction effects are all captured in the off-diagonal terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.dependence_plot(\n",
    "    (\"Age\", \"Age\"),\n",
    "    shap_interaction_values, X.iloc[:2000,:],\n",
    "    display_features=X_display.iloc[:2000,:]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we plot the interaction effects involving age. These effects capture all of the vertical dispersion that was present in the original SHAP plot but is missing from the main effects plot above. The plot below involving age and sex shows that the sex-based death risk gap varies by age and peaks at age 60."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.dependence_plot(\n",
    "    (\"Age\", \"Sex\"),\n",
    "    shap_interaction_values, X.iloc[:2000,:],\n",
    "    display_features=X_display.iloc[:2000,:]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.dependence_plot(\n",
    "    (\"Age\", \"Systolic BP\"),\n",
    "    shap_interaction_values, X.iloc[:2000,:],\n",
    "    display_features=X_display.iloc[:2000,:]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.dependence_plot(\n",
    "    (\"Age\", \"White blood cells\"),\n",
    "    shap_interaction_values, X.iloc[:2000,:],\n",
    "    display_features=X_display.iloc[:2000,:]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.dependence_plot(\n",
    "    (\"Age\", \"Poverty index\"),\n",
    "    shap_interaction_values, X.iloc[:2000,:],\n",
    "    display_features=X_display.iloc[:2000,:]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.dependence_plot(\n",
    "    (\"Age\", \"BMI\"),\n",
    "    shap_interaction_values, X.iloc[:2000,:],\n",
    "    display_features=X_display.iloc[:2000,:]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.dependence_plot(\n",
    "    (\"Age\", \"Serum magnesium\"),\n",
    "    shap_interaction_values, X.iloc[:2000,:],\n",
    "    display_features=X_display.iloc[:2000,:]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we show a couple examples with systolic blood pressure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.dependence_plot(\n",
    "    (\"Systolic BP\", \"Systolic BP\"),\n",
    "    shap_interaction_values, X.iloc[:2000,:],\n",
    "    display_features=X_display.iloc[:2000,:]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.dependence_plot(\n",
    "    (\"Systolic BP\", \"Age\"),\n",
    "    shap_interaction_values, X.iloc[:2000,:],\n",
    "    display_features=X_display.iloc[:2000,:]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.dependence_plot(\n",
    "    (\"Systolic BP\", \"Age\"),\n",
    "    shap_interaction_values, X.iloc[:2000,:],\n",
    "    display_features=X_display.iloc[:2000,:]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pylab as pl\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = np.abs(shap_interaction_values).sum(0)\n",
    "for i in range(tmp.shape[0]):\n",
    "    tmp[i,i] = 0\n",
    "inds = np.argsort(-tmp.sum(0))[:50]\n",
    "tmp2 = tmp[inds,:][:,inds]\n",
    "pl.figure(figsize=(12,12))\n",
    "pl.imshow(tmp2)\n",
    "pl.yticks(range(tmp2.shape[0]), X.columns[inds], rotation=50.4, horizontalalignment=\"right\")\n",
    "pl.xticks(range(tmp2.shape[0]), X.columns[inds], rotation=50.4, horizontalalignment=\"left\")\n",
    "pl.gca().xaxis.tick_top()\n",
    "pl.show()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "tomer",
   "language": "python",
   "name": "tomer"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
